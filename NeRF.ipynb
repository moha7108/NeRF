{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd82b4e-c7a3-42b2-bce1-38ad788276bb",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c57b3-0fbd-4807-9aaa-a9fed37a22b6",
   "metadata": {},
   "source": [
    "#### What Problem are we trying to solve?\n",
    "- View Synthesis generation of images from new and continuous view points \n",
    "\n",
    "### What are Neural Randiance Fields?\n",
    "\n",
    "- 3d neural network encoding for continous viewpoints, we want to over fit a neural network to a single point- \n",
    "- The weights of the Neural Networks represent the Scene\n",
    "\n",
    "\n",
    "#### Representation of a scene as a continous 5D function (NN)\n",
    "\n",
    "***Input:***\n",
    "- $$(x,y,z,\\theta, \\phi)$$\n",
    "    - $$(x,y,z) - Coordinates$$\n",
    "    - $$(\\theta, \\phi) - viewing\\;direction$$\n",
    "\n",
    "***Output:***\n",
    "- $$(r,g,b,\\sigma)$$\n",
    "    - $$(r,g,b) - color\\;channels$$\n",
    "    - $$\\sigma - density$$\n",
    "    \n",
    "#### Techniques used\n",
    "- Volume rendering( to accumulate samples from a scene representation along rays)\n",
    "\n",
    "1) march camera rays through the scene to generate a sampled set of 3D points\n",
    "2) Use those points and their corresponding 2D viewing directions as input to the newral network to produce an output set of colors and densities\n",
    "3) use classical volume rendering techniques to naturally accumulate those colors and densities into a 2d image.\n",
    "\n",
    "- naturally differentiable, can use gradient descent to optimize model by minimizing the error between each obserbed image and the corresponding views rendered from our representation\n",
    "- Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content.\n",
    "\n",
    "#### Training the Neural network\n",
    "\n",
    "- optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently highresolution representation and is inefficient in the required number of samples per camera ray.\n",
    "\n",
    "- transforming input 5D coordinates with a **positional encoding** that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation\n",
    "\n",
    "#### Advantages\n",
    "- can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images\n",
    "- overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions.\n",
    "\n",
    "##### 3 main contributions\n",
    "- An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks.\n",
    "- A differentiable rendering procedure based on classical volume rendering techniques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLPâ€™s capacity towards space with visible scene content.\n",
    "- A positional encoding to map each input 5D coordinate into a higher dimensional space, which enables us to successfully optimize neural radiance fields to represent high-frequency scene content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898bf2b-0c70-4e61-b976-2d51154f65e2",
   "metadata": {},
   "source": [
    "## Neural Scene Representation\n",
    "\n",
    "### Neural 3D shape representation\n",
    "- the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map xyz coordinates to signed distance functions or occupancy fields\n",
    "    - However, these models are limited by their requirement of access to ground truth 3D geometry typically obtained from synthetic 3D shape datasets such as ShapeNet\n",
    "    \n",
    "- relaxed models\n",
    "    - 3D occupancy fields\n",
    "    - \n",
    "    \n",
    "### View synthesis and image-based rendering\n",
    "\n",
    "- simple light field sample interpolation techniques (Dense samples required)\n",
    "- One popular class of approaches uses mesh-based representations of scenes with either diffuse or view-dependent appearance (sparser view samples)\n",
    "    - However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization, which is typically unavailable for unconstrained real-world scenes.\n",
    "#### Volumetric Representation\n",
    "- input rgb image,Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods.\n",
    "- voxel coloring\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca389b6-98a3-4abc-b1c3-b4119dbafcc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66a73598-d90d-4d58-bd8c-ba581b3bd194",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa76a2-3f0a-475d-93d7-b53ff48f675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8e6c7-ce85-4ab1-8661-582130b8a823",
   "metadata": {},
   "source": [
    "## Defining the Neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58931dc5-12b7-4ec5-af15-6a9d9b18e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):\n",
    "        super(NerfModel, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
    "\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction)\n",
    "        h = self.block1(emb_x)\n",
    "        tmp = self.block2(torch.cat((h, emb_x), dim=1))\n",
    "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1])\n",
    "        h = self.block3(torch.cat((h, emb_d), dim=1))\n",
    "        c = self.block4(h)\n",
    "        return c, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd4975-7f1d-45be-a9a3-bf97a06cf316",
   "metadata": {},
   "source": [
    "## Functions for sampling Rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0e9d5-d43a-4458-a18e-905078428a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accumulated_transmittance(alphas):\n",
    "    at = torch.cumprod(alphas,1)\n",
    "    \n",
    "    return torch.cat((torch.ones((at.shape[0],1), device=alphas.device), at[:,:-1]), dim=1)\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device = device).expand(ray_origins.shape[0], nb_bins)\n",
    "    \n",
    "    mid = (t[:,:-1] + t[:,1:]) / 2\n",
    "    lower = torch.cat((t[:,:1], mid),-1)\n",
    "    upper = torch.cat((mid,t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device = device)\n",
    "    t = lower + (upper - lower) * u\n",
    "    \n",
    "    delta = torch.cat((t[:,1:] - t[:,:-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0],1)), -1)\n",
    "    \n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0,1)\n",
    "    \n",
    "    colors, sigma = nerf_model(x.reshape(-1,3), ray_directions.reshape(-1,3))\n",
    "    colors = colors.reshape(x.shape)\n",
    "    sigma = sigma.reshape(x.shape[:-1])\n",
    "    \n",
    "    alpha = 1 - torch.exp(-sigma*delta)\n",
    "    weights = compute_accumulated_transmittance(1-alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights*colors).sum(dim=1)\n",
    "    weight_sum = weights.sum(-1).sum(-1)\n",
    "    \n",
    "    return c + 1 - weight_sum.unsqueeze(-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859830ca-d725-4bd3-99e2-27bed1cb94f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function for training and sampling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe7b7f-9a07-418c-9498-8ba0d76684a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(nerf_model, optimizer, scheduler, data_loader, device='cpu', hn=0, hf=1, nb_epochs=int(1e5), nb_bins=192, H=400, W=400,render_each_epoch=False):\n",
    "    training_loss = []\n",
    "    for _ in trange(nb_epochs):\n",
    "        for batch in tqdm(data_loader):\n",
    "            \n",
    "            ray_origins = batch[:,:3].to(device)\n",
    "            ray_directions = batch[:,3:6].to(device)\n",
    "            ground_truth_px_values = batch[:,6:].to(device)\n",
    "            \n",
    "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            loss = ((ground_truth_px_values - regenerated_px_values)**2).sum()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        scheduler.step()\n",
    "        \n",
    "        if render_each_epoch:\n",
    "            print('Saving New Viewpoints')\n",
    "            for img_index in range(200):\n",
    "                test(hn, hf, testing_dataset, img_index=img_index, nb_bins =nb_bins, H=H, W=W,device=device)\n",
    "    \n",
    "    return training_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(hn, hf, dataset, chunk_size=10, img_index=0, nb_bins=192,H=400, W=400, device = 'cpu'):\n",
    "    ray_origins = dataset[img_index*H*W: (img_index+1)*H*W, :3]\n",
    "    ray_directions = dataset[img_index *H*W: (img_index+1)*H*W, 3:6]\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(int(np.ceil(H/chunk_size))):\n",
    "        ray_origins_ = ray_origins[i*W*chunk_size: (i+1)*W*chunk_size].to(device)\n",
    "        ray_directions_ = ray_directions[i*W*chunk_size: (i+1)*W*chunk_size].to(device)\n",
    "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values)\n",
    "        \n",
    "    img = torch.cat(data).data.cpu().numpy().reshape(H,W,3)\n",
    "    \n",
    "    plt.figure();\n",
    "    plt.imshow(img);\n",
    "    plt.savefig(f'novel_views/img_{img_index}.png', bbox_inches='tight');\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376ce71-1dba-4a35-8327-d43ef72e54ec",
   "metadata": {},
   "source": [
    "## Load and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2148f2-f209-4b7b-89dd-a0cf3ca0cffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "training_dataset = torch.from_numpy(np.load('./data/training_data.pkl', allow_pickle=True))\n",
    "testing_dataset = torch.from_numpy(np.load('./data/testing_data.pkl', allow_pickle=True))\n",
    "\n",
    "model = NerfModel(hidden_dim=256).to(device)\n",
    "\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2,4,8], gamma=0.5)\n",
    "\n",
    "data_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True)\n",
    "train(model, model_optimizer, scheduler, data_loader, nb_epochs=16, device=device, hn=2, hf=6, nb_bins=192, H=400, W=400, render_each_epoch=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90ff00-397b-45fd-8033-0496cbd36ff6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sample the model to generate a new View Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c58dd-6b20-4500-bef8-47c1570d8590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_index in range(200):\n",
    "    test(hn=2, hf=6, dataset=testing_dataset, chunk_size=10, img_index=img_index, nb_bins=192, H=400, W=400, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7627fac-2469-490d-aef3-0248e54bda50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
